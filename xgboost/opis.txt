Opis metody: XGBoost

XGBoost (Extreme Gradient Boosting) to zaawansowana technika uczenia maszynowego oparta na metodzie boosting,
ktÃ³ra iteracyjnie buduje modele (gÅ‚Ã³wnie drzewa decyzyjne), korygujÄ…c bÅ‚Ä™dy poprzednich.
Celem XGBoost jest minimalizacja funkcji straty przy jednoczesnej regularyzacji, co pozwala na wysokÄ… skutecznoÅ›Ä‡ i odpornoÅ›Ä‡ na przeuczenie.

Zalety XGBoost:

    Bardzo wydajny i szybki, dziÄ™ki optymalizacjom sprzÄ™towym i rÃ³wnolegÅ‚emu uczeniu.

    Wysoka dokÅ‚adnoÅ›Ä‡ na rÃ³Å¼norodnych problemach klasyfikacyjnych i regresyjnych.

    ObsÅ‚uguje brakujÄ…ce dane oraz umoÅ¼liwia zaawansowane opcje regularyzacji (L1, L2).

    Dobry w problemach z niezrÃ³wnowaÅ¼onymi klasami â€“ moÅ¼na uÅ¼ywaÄ‡ scale_pos_weight.

Wady XGBoost:

    Jest bardziej zÅ‚oÅ¼ony niÅ¼ np. Random Forest â€” trudniejszy do interpretacji.



PrÃ³g = 5

    DokÅ‚adnoÅ›Ä‡ testowa: 95.8%, F1: 0.979

    Model bardzo dobrze oddziela prÃ³bki gorszej i lepszej jakoÅ›ci przy progu 5 â€” dane sÄ… dobrze rozdzielone.

    Wysoka precyzja i czuÅ‚oÅ›Ä‡, takÅ¼e na zbiorze testowym â€” model nie jest przeuczony, ale dobrze uogÃ³lnia.

PrÃ³g = 6

    DokÅ‚adnoÅ›Ä‡ testowa: 78.8%, F1: 0.839

    Model nadal dziaÅ‚a dobrze, choÄ‡ dokÅ‚adnoÅ›Ä‡ i F1 spadajÄ….

    Wskazuje to, Å¼e granica jakoÅ›ci = 6 jest trudniejsza do rozrÃ³Å¼nienia â€“ cechy nie rozdzielajÄ… juÅ¼ tak dobrze klas.

PrÃ³g = 7

    DokÅ‚adnoÅ›Ä‡ testowa: 87.2%, ale F1: 0.603

    Mimo wysokiej dokÅ‚adnoÅ›ci, czuÅ‚oÅ›Ä‡ na testowych spada do 0.50 â€“ model pomija wiele prawdziwych przypadkÃ³w klasy â€dobre winoâ€.

    MoÅ¼e to wskazywaÄ‡ na niedostatecznÄ… liczbÄ™ przykÅ‚adÃ³w w klasie â‰¥ 7 lub brak wyraÅºnych rÃ³Å¼nic w cechach.

ğŸ§  Wnioski

    XGBoost sprawdza siÄ™ najlepiej przy klasyfikacji z progiem jakoÅ›ci 5 â€“ klasy sÄ… Å‚atwo rozrÃ³Å¼nialne.

    Dla wyÅ¼szych progÃ³w (np. 7) model ma trudnoÅ›ci z generalizacjÄ… â€” moÅ¼na sprÃ³bowaÄ‡ dostosowaÄ‡ scale_pos_weight, zastosowaÄ‡ inne metryki (np. AUC),
    lub zastosowaÄ‡ oversampling mniejszoÅ›ciowej klasy (np. SMOTE).

    Normalizacja nie miaÅ‚a wpÅ‚ywu na wyniki, co jest typowe dla drzewiastych modeli (jak XGBoost), ktÃ³re nie wymagajÄ… skalowania danych.